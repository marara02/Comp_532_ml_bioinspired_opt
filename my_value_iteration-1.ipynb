{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lsu1oL3hTGtu"
      },
      "source": [
        "## A simple game\n",
        "\n",
        "We need to define actions, states, state transitions, rewards and the discount factor.\n",
        "\n",
        "An MDP is a 5-tuple, $\\langle S, A, R, P, \\gamma \\rangle$\n",
        "\n",
        "--16 states\n",
        "\n",
        "--4 actions\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3r_bLH54S1_k",
        "outputId": "f7c84269-2a66-49a7-d67d-44296dbecbcb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            " [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            " [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            " [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            " [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            " [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
            "\n",
            " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
            "\n",
            " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "np.set_printoptions(threshold=np.inf)\n",
        "\n",
        "\n",
        "# Define the gridworld environment\n",
        "n_states = 16  # Total number of states in the grid (4x4 grid)\n",
        "n_actions = 4  # Total number of possible actions: up, down, left, right\n",
        "# Initialize the transition probabilities matrix\n",
        "# It is a 3D array where P[s, a, s'] represents the probability of transitioning from state s to state s' given action a\n",
        "P = np.zeros((n_states, n_actions, n_states))\n",
        "# Initialize the rewards matrix\n",
        "# R[s, a, s'] represents the reward received when transitioning from state s to state s' given action a\n",
        "R = np.zeros((n_states, n_actions, n_states))\n",
        "gamma = 0.9  # Discount factor for future rewards\n",
        "\n",
        "# Fill in the transition probabilities and rewards for each state and action\n",
        "for s in range(n_states):  # Loop over all states\n",
        "    for a in range(n_actions):  # Loop over all actions\n",
        "        if s == 0 or s == 15:  # If the current state is a terminal state (0 or 15)\n",
        "            P[s, a, s] = 1  # Stay in the same state with probability 1\n",
        "        else:  # If the current state is not terminal\n",
        "            if a == 0:  # Up action\n",
        "                s_prime = s - 4  # Calculate the new state after moving up\n",
        "            elif a == 1:  # Down action\n",
        "                s_prime = s + 4  # Calculate the new state after moving down\n",
        "            elif a == 2:  # Left action\n",
        "                s_prime = s - 1  # Calculate the new state after moving left\n",
        "            else:  # Right action\n",
        "                s_prime = s + 1  # Calculate the new state after moving right\n",
        "\n",
        "            # Ensure the new state is within bounds\n",
        "            if s_prime < 0:\n",
        "                s_prime = 0  # If moving out of grid upwards, stay at the top\n",
        "            if s_prime > 15:\n",
        "                s_prime = 15  # If moving out of grid downwards, stay at the bottom\n",
        "\n",
        "            # Assign rewards based on the new state\n",
        "            if s_prime == 0:\n",
        "                R[s, a, s_prime] = -1  # Penalty for moving to the start state\n",
        "            elif s_prime == 15:\n",
        "                R[s, a, s_prime] = 10  # Reward for reaching the goal state\n",
        "            else:\n",
        "                R[s, a, s_prime] = -1  # Penalty for non-terminal states to encourage reaching the goal\n",
        "\n",
        "            P[s, a, s_prime] = 1  # Set the transition probability to 1 for the determined action\n",
        "\n",
        "print(P)\n",
        "# P[s, a, s_prime]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NW6dD3sSVDCc"
      },
      "source": [
        "## Solution - DP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MbgBxKWUVCuY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6ee09a6-7bc4-49b0-b4f4-8df9d9ac03d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]]\n",
            "(State  0 ) Actions [0.25 0.25 0.25 0.25]\n",
            "(State  1 ) Actions [0.25 0.25 0.25 0.25]\n",
            "(State  2 ) Actions [0.25 0.25 0.25 0.25]\n",
            "(State  3 ) Actions [0.25 0.25 0.25 0.25]\n",
            "(State  4 ) Actions [0.25 0.25 0.25 0.25]\n",
            "(State  5 ) Actions [0.25 0.25 0.25 0.25]\n",
            "(State  6 ) Actions [0.25 0.25 0.25 0.25]\n",
            "(State  7 ) Actions [0.25 0.25 0.25 0.25]\n",
            "(State  8 ) Actions [0.25 0.25 0.25 0.25]\n",
            "(State  9 ) Actions [0.25 0.25 0.25 0.25]\n",
            "(State  10 ) Actions [0.25 0.25 0.25 0.25]\n",
            "(State  11 ) Actions [0.25 0.25 0.25 0.25]\n",
            "(State  12 ) Actions [0.25 0.25 0.25 0.25]\n",
            "(State  13 ) Actions [0.25 0.25 0.25 0.25]\n",
            "(State  14 ) Actions [0.25 0.25 0.25 0.25]\n",
            "(State  15 ) Actions [0.25 0.25 0.25 0.25]\n",
            "Initial value function:\n",
            "[[0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]]\n",
            "Final value function:\n",
            "[[ 0.         -2.36003667 -2.84272002 -2.80977835]\n",
            " [-2.8029887  -3.20188587 -3.02005022 -2.39774911]\n",
            " [-2.00161733 -1.60308234 -0.53564463  1.61922876]\n",
            " [ 2.35218862  3.05878273  5.06770607  0.        ]]\n"
          ]
        }
      ],
      "source": [
        "# Define an arbitrary policy for now\n",
        "# This policy is uniformly random, meaning at each state, each action is equally likely.\n",
        "policy = np.ones((n_states, n_actions)) / n_actions\n",
        "print(policy)\n",
        "\n",
        "for s in range(n_states):\n",
        "  print(\"(State \", s, \") Actions\", policy[s])\n",
        "\n",
        "# Initialize the value function to zeros for all states.\n",
        "# V[s] represents the expected return starting from state s and following the given policy.\n",
        "V = np.zeros(n_states)\n",
        "print(\"Initial value function:\")\n",
        "print(V.reshape(4, 4))  # Reshape for better readability, assuming a 4x4 grid.\n",
        "\n",
        "tolerance = 1e-6  # Convergence tolerance, stops the algorithm when changes are small.\n",
        "\n",
        "# Policy Evaluation Loop\n",
        "while True:\n",
        "    delta = 0  # Initialize delta to track the maximum change in value function across all states in an iteration.\n",
        "    for s in range(n_states):  # Iterate over all states.\n",
        "        v = V[s]  # Store the current value of the state.\n",
        "        bellman_update = 0  # This will accumulate the sum of expected returns.\n",
        "        for a in range(n_actions):  # Iterate over all actions.\n",
        "            for s_prime in range(n_states):  # Iterate over all possible next states.\n",
        "                # Calculate the expected return using the Bellman equation for policy evaluation.\n",
        "                bellman_update += policy[s, a] * P[s, a, s_prime] * (R[s, a, s_prime] + gamma * V[s_prime])\n",
        "        V[s] = bellman_update  # Update the value function with the computed expected return.\n",
        "        delta = max(delta, abs(v - V[s]))  # Update delta with the largest change in value function for any state.\n",
        "    if delta < tolerance:  # If the maximum change is less than the tolerance, the value function has converged.\n",
        "        break\n",
        "\n",
        "print(\"Final value function:\")\n",
        "print(V.reshape(4, 4))  # Print the final value function, reshaped into the 4x4 grid."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hY6O4iKMaj5a"
      },
      "source": [
        "## Q-learning algorithm\n",
        "\n",
        "## Q-Learning Overview\n",
        "\n",
        "Q-Learning is a model-free reinforcement learning algorithm used to inform an agent on how to act optimally in a given environment by learning the value of actions in states. It does not require a model of the environment (hence \"model-free\"), and it can handle problems with stochastic transitions and rewards, without needing adaptations.\n",
        "\n",
        "### Key Concepts\n",
        "\n",
        "- **Q-Value (Action-Value):** Represents the value of taking a particular action in a particular state, given that the agent will follow the optimal policy thereafter. The Q-value for a state-action pair \\((s, a)\\), denoted as \\(Q(s, a)\\), is the expected return (total discounted future reward) starting from state \\(s\\), taking action \\(a\\), and thereafter following an optimal policy.\n",
        "\n",
        "- **Policy:** A strategy that the agent employs to determine the next action based on the current state. The optimal policy \\(\\pi^*\\) maximizes the expected return from all states.\n",
        "\n",
        "- **Reward:** A signal received from the environment in response to an action, indicating the immediate benefit of that action.\n",
        "\n",
        "- **State-Action Pairs:** The combination of a state and an action within that state, often represented as \\((s, a)\\).\n",
        "\n",
        "### The Q-Learning Algorithm\n",
        "\n",
        "The Q-learning algorithm updates Q-values using the Bellman equation as follows:\n",
        "\n",
        "$$\n",
        "Q(s, a) \\leftarrow Q(s, a) + \\alpha \\left[ r + \\gamma \\max_{a'}Q(s', a') - Q(s, a) \\right]\n",
        "$$\n",
        "\n",
        "\n",
        "where:\n",
        "- $s$ is the current state,\n",
        "- $a$ is the current action,\n",
        "- $s'$ is the new state after taking action $a$,\n",
        "- $r$ is the reward received after taking action $a$ in state $s$,\n",
        "- $\\alpha$ is the learning rate,\n",
        "- $\\gamma$ is the discount factor, and\n",
        "- $\\max_{a'}Q(s', a')$ is the maximum predicted Q-value for the next state $s'$, representing the best future reward that can be obtained.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uzFly1mtafAd",
        "outputId": "b8daa693-100a-4748-c23d-15610b6b25bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            " [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            " [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            " [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            " [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            " [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
            "\n",
            " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
            "\n",
            " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]]\n",
            "Q [[0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]]\n",
            "Optimal policy:\n",
            "[[0 3 3 1]\n",
            " [2 3 3 1]\n",
            " [2 1 3 1]\n",
            " [1 1 1 0]]\n"
          ]
        }
      ],
      "source": [
        "# Initialize the Q-values matrix with zeros for all state-action pairs.\n",
        "# Q[s, a] represents the value of taking action a in state s.\n",
        "print(P)\n",
        "Q = np.zeros((n_states, n_actions))\n",
        "print(\"Q\", Q)\n",
        "n_episodes = 10000  # Number of episodes for the agent to learn from.\n",
        "alpha = 0.1  # Learning rate, determining how much new information overrides old information.\n",
        "epsilon = 0.1  # Epsilon-greedy exploration probability\n",
        "# Loop over each episode.\n",
        "for episode in range(n_episodes):\n",
        "    # Randomly select an initial state for the beginning of the episode.\n",
        "    s = np.random.randint(n_states)\n",
        "    # Continue the episode until reaching a terminal state (0 or 15 in this gridworld).\n",
        "    while s not in [0, 15]:\n",
        "        # Epsilon-greedy action selection:\n",
        "        # With probability epsilon, select a random action (exploration).\n",
        "        if np.random.uniform() < epsilon:\n",
        "            a = np.random.randint(n_actions)\n",
        "        else:\n",
        "            # With probability 1 - epsilon, select the action with the highest Q-value.\n",
        "            a = np.argmax(Q[s, :])\n",
        "        # Take the selected action, observe the next state and reward.\n",
        "        # The next state is chosen based on the transition probabilities P[s, a, :].\n",
        "        s_prime = np.random.choice(range(n_states), p=P[s, a, :])\n",
        "        r = R[s, a, s_prime]  # The reward received for the (s, a, s') transition.\n",
        "        # Update the Q-value for the current state-action pair using the Q-learning formula.\n",
        "        # This incorporates the immediate reward and the discounted value of the best future action.\n",
        "        Q[s, a] = Q[s, a] + alpha * (r + gamma * np.max(Q[s_prime, :]) - Q[s, a])\n",
        "        # Move to the next state.\n",
        "        s = s_prime\n",
        "\n",
        "# After learning, extract the optimal policy from the Q-values.\n",
        "# The optimal action for each state is the one with the highest Q-value.\n",
        "policy = np.argmax(Q, axis=1)\n",
        "# print(\"updated Q: \", Q)\n",
        "print(\"Optimal policy:\")\n",
        "# Print the optimal policy, reshaped into the grid format for easy visualization.\n",
        "print(policy.reshape(4, 4))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}